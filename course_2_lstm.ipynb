{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: text_corpus.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (5246 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 206906 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 200 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=47338605\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=118\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 206889 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=23496736\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 445841 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 206889\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 440740\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 440740 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=174316 obj=11.8734 num_tokens=1037849 num_tokens/piece=5.95384\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=153657 obj=9.51373 num_tokens=1042146 num_tokens/piece=6.78229\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=115222 obj=9.49154 num_tokens=1084347 num_tokens/piece=9.41094\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=115163 obj=9.47964 num_tokens=1084616 num_tokens/piece=9.41809\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=86371 obj=9.53686 num_tokens=1144282 num_tokens/piece=13.2485\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=86368 obj=9.52491 num_tokens=1144213 num_tokens/piece=13.2481\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=64776 obj=9.60992 num_tokens=1211947 num_tokens/piece=18.7098\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=64776 obj=9.59298 num_tokens=1211773 num_tokens/piece=18.7071\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=48582 obj=9.70726 num_tokens=1283887 num_tokens/piece=26.4272\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=48581 obj=9.68637 num_tokens=1283864 num_tokens/piece=26.4273\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=36435 obj=9.82536 num_tokens=1356267 num_tokens/piece=37.2243\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=36435 obj=9.79957 num_tokens=1356170 num_tokens/piece=37.2216\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=27326 obj=9.97025 num_tokens=1430376 num_tokens/piece=52.3449\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27326 obj=9.93869 num_tokens=1430382 num_tokens/piece=52.3451\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20494 obj=10.1425 num_tokens=1506094 num_tokens/piece=73.4895\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20494 obj=10.1043 num_tokens=1506154 num_tokens/piece=73.4924\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15370 obj=10.3371 num_tokens=1584483 num_tokens/piece=103.089\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15370 obj=10.2925 num_tokens=1584672 num_tokens/piece=103.102\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11527 obj=10.5611 num_tokens=1666791 num_tokens/piece=144.599\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11527 obj=10.5079 num_tokens=1666806 num_tokens/piece=144.6\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8645 obj=10.819 num_tokens=1748157 num_tokens/piece=202.216\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8645 obj=10.7575 num_tokens=1748461 num_tokens/piece=202.251\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6483 obj=11.1029 num_tokens=1837200 num_tokens/piece=283.387\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6483 obj=11.0326 num_tokens=1837369 num_tokens/piece=283.413\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5500 obj=11.2469 num_tokens=1886351 num_tokens/piece=342.973\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5500 obj=11.2036 num_tokens=1886501 num_tokens/piece=343\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: bpe.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "with open(\"text_corpus.txt\", \"w\") as f:\n",
    "    for line in texts:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(input=\"text_corpus.txt\", model_prefix=\"bpe\", vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire SentencePiece : 5000\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"bpe.model\")\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(\"Taille du vocabulaire SentencePiece :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [sp.encode(text, out_type=int) for text in texts]\n",
    "token_to_idx = {i: i for i in range(vocab_size)}\n",
    "idx_to_token = {i: sp.id_to_piece(i) for i in range(vocab_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, seq_length=50):\n",
    "        self.seq_length = seq_length\n",
    "        self.tokens = [sp.encode(text, out_type=int) for text in texts]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_seq = self.tokens[idx]\n",
    "        if len(token_seq) > self.seq_length + 1:\n",
    "            token_seq = token_seq[:self.seq_length + 1]\n",
    "        if len(token_seq) < self.seq_length + 1:\n",
    "            token_seq += [0] * (self.seq_length + 1 - len(token_seq))\n",
    "\n",
    "        input_ids = torch.tensor(token_seq[:-1], dtype=torch.long)\n",
    "        target_ids = torch.tensor(token_seq[1:], dtype=torch.long)\n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "batch_size = 32\n",
    "\n",
    "dataset = TextDataset(texts, seq_length)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, model_type=\"LSTM\"):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if model_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        if hidden is None:\n",
    "            output, hidden = self.rnn(x)\n",
    "        else:\n",
    "            output, hidden = self.rnn(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "model_type = \"LSTM\"\n",
    "\n",
    "model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for input_ids, target_ids in progress_bar:\n",
    "            input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
    "            target_ids = torch.clamp(target_ids, 0, vocab_size - 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(input_ids)\n",
    "            loss = criterion(output.view(-1, vocab_size), target_ids.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 313/313 [01:07<00:00,  4.65it/s, loss=6.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.763984471464309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 313/313 [01:09<00:00,  4.53it/s, loss=5.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 6.059464445510231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 313/313 [01:08<00:00,  4.60it/s, loss=5.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 5.680186483425835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 313/313 [01:01<00:00,  5.07it/s, loss=5.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 5.400537746782882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 313/313 [01:00<00:00,  5.19it/s, loss=4.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 5.158731810962811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 313/313 [01:04<00:00,  4.88it/s, loss=4.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 4.937488930674787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 313/313 [01:07<00:00,  4.64it/s, loss=4.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 4.7269295823459805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 313/313 [01:18<00:00,  4.01it/s, loss=4.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 4.525204314210544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 313/313 [01:05<00:00,  4.79it/s, loss=4.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 4.329140427013556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 313/313 [01:10<00:00,  4.44it/s, loss=4.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 4.137776145538964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 313/313 [01:06<00:00,  4.72it/s, loss=4.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 3.9513134781164103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 313/313 [00:59<00:00,  5.23it/s, loss=3.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 3.769791711252718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 313/313 [00:58<00:00,  5.32it/s, loss=3.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 3.590873859180048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 313/313 [00:57<00:00,  5.46it/s, loss=3.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 3.4165415215416077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 313/313 [00:58<00:00,  5.34it/s, loss=3.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 3.24602595761942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 313/313 [00:59<00:00,  5.22it/s, loss=3.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 3.0809773598996975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 313/313 [01:03<00:00,  4.93it/s, loss=2.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 2.920178698274655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 313/313 [01:01<00:00,  5.07it/s, loss=2.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 2.764998540329857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 313/313 [01:00<00:00,  5.17it/s, loss=2.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 2.613964153174013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 313/313 [00:58<00:00,  5.36it/s, loss=2.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 2.4700150565979198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 313/313 [00:58<00:00,  5.33it/s, loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 2.329532549404108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 313/313 [01:02<00:00,  5.00it/s, loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 2.195609826630297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 313/313 [00:58<00:00,  5.33it/s, loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 2.0665937574526754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 313/313 [00:59<00:00,  5.25it/s, loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 1.9446976261017042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 313/313 [00:58<00:00,  5.34it/s, loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 1.8273235845108764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 313/313 [00:59<00:00,  5.30it/s, loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 1.7156557499791105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 313/313 [00:58<00:00,  5.31it/s, loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 1.6086390795418248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 313/313 [01:00<00:00,  5.21it/s, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 1.5055517891344552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 313/313 [00:58<00:00,  5.31it/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 1.4071651864737367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 313/313 [01:01<00:00,  5.12it/s, loss=1.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 1.3174805203184914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, train_loader, optimizer, criterion, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2014 campaign passed a zoopated from 1873 1 in Florida last Toronto knows several new hot in Paris State Adxoinped in South Park fell on Boeah, the long-looked segi man since he failed to\n"
     ]
    }
   ],
   "source": [
    "def generate_text_bpe(model, tokenizer, start_text, max_length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor([tokenizer.encode(start_text, out_type=int)], dtype=torch.long)\n",
    "    hidden = None\n",
    "    generated_tokens = input_ids.tolist()[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = model(input_ids, hidden)\n",
    "            probabilities = torch.nn.functional.softmax(output[:, -1, :] / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            generated_tokens.append(next_token)\n",
    "            input_ids = torch.tensor([[next_token]])\n",
    "\n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "start_text = \"The\"\n",
    "print(generate_text_bpe(model, sp, start_text, temperature=1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
